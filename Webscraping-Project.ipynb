{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Library\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Url of the website to scrape\n",
    "url = \"https://lodzka.policja.gov.pl/ld/form/63,Statystyki-dnia.html?page=0\"\n",
    "# List to store scraped values\n",
    "values = list()\n",
    "# Initialization of page index\n",
    "i = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling try-except block to prevent from index error\n",
    "try:\n",
    "    # Loop statement: we loop through all url's that contain word \"Statystyki\"\n",
    "    while \"Statystyki\" in url:\n",
    "        # Send respond to the server with specified url\n",
    "        response = requests.get(url = url)\n",
    "        # Assert whether the status of response is 200 which means: Successful response\n",
    "        assert response.status_code == 200\n",
    "        # Get html content of the website\n",
    "        html = response.text\n",
    "        # Parse the html content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        # Extract the headers of the tables (they are stored in: table tag -> attribute class with value table-listing margin_b20 -> tag th)\n",
    "        headers = [header.text for header in soup.findAll(\"table\", {\"class\":\"table-listing margin_b20\"})[0].find_all({\"th\"})]\n",
    "        # Extend the values list about scraped values that are in format of comprehensive list\n",
    "        values.extend([value.text for value in soup.findAll(\"table\", {\"class\":\"table-listing margin_b20\"})[0].find_all({\"td\"})])\n",
    "        # Find all hypertext references (they were stored in: div tag -> attribute id with value meni_strony -> tag a)\n",
    "        pages_linked = [page.get(\"href\") for page in soup.findAll(\"div\", {\"id\":\"meni_strony\"})[0].findAll(\"a\")]\n",
    "        # As there are many hyperlinks for every page, we choose the page that has index about one higher that one we are on\n",
    "        pages_next = list(filter(lambda x: x if \"page=\"+str(i+1) in x else 0, pages_linked))[0]\n",
    "        # Assign new url to url variable\n",
    "        url = url.split(\"/ld/\")[0]+pages_next\n",
    "        # Increment the page index\n",
    "        i += 1\n",
    "        # We use time sleep with gamma distribution (right-skewed distribution) to prevent being recognized as a bot\n",
    "        time.sleep(np.ceil(np.random.gamma(2., 2.)))\n",
    "# When the IndexError occurs, we looped through all websites\n",
    "except IndexError:\n",
    "    print(\"All sites has been scraped\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists that will be then converted to rows in pandas dataframe\n",
    "observations = list()\n",
    "# Append list of observations for every day to observations list\n",
    "for i in range(int(len(values) / 7)):\n",
    "    observations.append(values[7*i:7*(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2954 entries, 0 to 2953\n",
      "Data columns (total 7 columns):\n",
      " #   Column                           Non-Null Count  Dtype         \n",
      "---  ------                           --------------  -----         \n",
      " 0   Data                             2954 non-null   datetime64[ns]\n",
      " 1   Zatrzymani na gorącym uczynku    2954 non-null   int64         \n",
      " 2   Zatrzymani poszukiwani           2954 non-null   int64         \n",
      " 3   Zatrzymani nietrzeźwi kierujący  2954 non-null   int64         \n",
      " 4   Wypadki drogowe                  2954 non-null   int64         \n",
      " 5   Zabici w wypadkach               2954 non-null   int64         \n",
      " 6   Ranni w wypadkach                2954 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(6)\n",
      "memory usage: 161.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Create pandas dataframe\n",
    "data = pd.DataFrame(observations)\n",
    "# Change headers to scraped ones\n",
    "data.columns = headers\n",
    "# Convert column \"Data\" to datetime\n",
    "data[\"Data\"] = data[\"Data\"].astype(\"datetime64[ns]\")\n",
    "# Create a function that converts all object columns to numeric data type\n",
    "def object_to_integer(dataframe):\n",
    "    \"\"\"Converts object columns to integer data type\"\"\"\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == \"object\":\n",
    "            dataframe[column] = dataframe[column].astype(\"int\")\n",
    "object_to_integer(data)\n",
    "data.info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
